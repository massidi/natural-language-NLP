{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "LLMLangChain",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "secret_OpenAI = UserSecretsClient().get_secret(\"OpenAI_KEY\")\n",
        "secret_Langchain = UserSecretsClient().get_secret(\"llmLangChain_test\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:12:27.848998Z",
          "iopub.execute_input": "2024-04-28T13:12:27.849443Z",
          "iopub.status.idle": "2024-04-28T13:12:28.078831Z",
          "shell.execute_reply.started": "2024-04-28T13:12:27.849406Z",
          "shell.execute_reply": "2024-04-28T13:12:28.077801Z"
        },
        "trusted": true,
        "id": "9VFDqDexbF-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "h5FZJUoebF-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export LANGCHAIN_TRACING_V2=true\n",
        "# export LANGCHAIN_API_KEY=secret_Langchain\n",
        "\n",
        "# # The below examples use the OpenAI API, though it's not necessary in general\n",
        "# export OPENAI_API_KEY=secret_OpenAI"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T08:24:42.692114Z",
          "iopub.execute_input": "2024-04-27T08:24:42.692413Z",
          "iopub.status.idle": "2024-04-27T08:24:42.704954Z",
          "shell.execute_reply.started": "2024-04-27T08:24:42.692386Z",
          "shell.execute_reply": "2024-04-27T08:24:42.70365Z"
        },
        "trusted": true,
        "id": "4bbZmIqZbF-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = secret_Langchain\n",
        "os.environ['OPENAI_API_KEY'] = secret_OpenAI\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:12:31.948317Z",
          "iopub.execute_input": "2024-04-28T13:12:31.948685Z",
          "iopub.status.idle": "2024-04-28T13:12:31.954834Z",
          "shell.execute_reply.started": "2024-04-28T13:12:31.948656Z",
          "shell.execute_reply": "2024-04-28T13:12:31.953445Z"
        },
        "trusted": true,
        "id": "jYmE_ziIbF--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:12:34.507452Z",
          "iopub.execute_input": "2024-04-28T13:12:34.507852Z",
          "iopub.status.idle": "2024-04-28T13:12:36.527269Z",
          "shell.execute_reply.started": "2024-04-28T13:12:34.507819Z",
          "shell.execute_reply": "2024-04-28T13:12:36.526108Z"
        },
        "trusted": true,
        "id": "5qwMMaxbbF--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:12:39.55824Z",
          "iopub.execute_input": "2024-04-28T13:12:39.558831Z",
          "iopub.status.idle": "2024-04-28T13:12:39.781938Z",
          "shell.execute_reply.started": "2024-04-28T13:12:39.558796Z",
          "shell.execute_reply": "2024-04-28T13:12:39.780962Z"
        },
        "trusted": true,
        "id": "nMbubZhzbF-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:13:26.421151Z",
          "iopub.execute_input": "2024-04-28T13:13:26.422046Z",
          "iopub.status.idle": "2024-04-28T13:13:28.305602Z",
          "shell.execute_reply.started": "2024-04-28T13:13:26.422007Z",
          "shell.execute_reply": "2024-04-28T13:13:28.304184Z"
        },
        "trusted": true,
        "id": "6PtI77OLbF-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:20:30.11624Z",
          "iopub.execute_input": "2024-04-28T13:20:30.116686Z",
          "iopub.status.idle": "2024-04-28T13:20:30.124332Z",
          "shell.execute_reply.started": "2024-04-28T13:20:30.116651Z",
          "shell.execute_reply": "2024-04-28T13:20:30.122822Z"
        },
        "trusted": true,
        "id": "L3yPMhxkbF-_",
        "outputId": "f81ea2e3-6a2e-48d4-fb0e-919e72a09531"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Document(page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(splits[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:20:38.190594Z",
          "iopub.execute_input": "2024-04-28T13:20:38.191007Z",
          "iopub.status.idle": "2024-04-28T13:20:38.195825Z",
          "shell.execute_reply.started": "2024-04-28T13:20:38.190975Z",
          "shell.execute_reply": "2024-04-28T13:20:38.194648Z"
        },
        "trusted": true,
        "id": "Om06nA2DbF_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DtUelilbF_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=OpenAIEmbeddings())\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:13:36.395469Z",
          "iopub.execute_input": "2024-04-28T13:13:36.395881Z",
          "iopub.status.idle": "2024-04-28T13:13:40.635655Z",
          "shell.execute_reply.started": "2024-04-28T13:13:36.395847Z",
          "shell.execute_reply": "2024-04-28T13:13:40.634497Z"
        },
        "trusted": true,
        "id": "FBUYNYBJbF_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
        "\n",
        "docs\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:20:45.738951Z",
          "iopub.execute_input": "2024-04-28T13:20:45.739353Z",
          "iopub.status.idle": "2024-04-28T13:20:45.927406Z",
          "shell.execute_reply.started": "2024-04-28T13:20:45.73931Z",
          "shell.execute_reply": "2024-04-28T13:20:45.92627Z"
        },
        "trusted": true,
        "id": "RdzXQMiObF_C",
        "outputId": "8d025c40-4894-4207-90f4-ad2be10c4765"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T11:09:40.877955Z",
          "iopub.execute_input": "2024-04-27T11:09:40.879053Z",
          "iopub.status.idle": "2024-04-27T11:09:40.893572Z",
          "shell.execute_reply.started": "2024-04-27T11:09:40.879006Z",
          "shell.execute_reply": "2024-04-27T11:09:40.892372Z"
        },
        "trusted": true,
        "id": "wfYY_JWObF_D",
        "outputId": "3ee144af-dcdf-42ac-df73-7b476276f025"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T11:10:56.06915Z",
          "iopub.execute_input": "2024-04-27T11:10:56.069657Z",
          "iopub.status.idle": "2024-04-27T11:10:56.103486Z",
          "shell.execute_reply.started": "2024-04-27T11:10:56.069618Z",
          "shell.execute_reply": "2024-04-27T11:10:56.102408Z"
        },
        "trusted": true,
        "id": "OlVkRtQAbF_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T11:10:59.60784Z",
          "iopub.execute_input": "2024-04-27T11:10:59.608664Z",
          "iopub.status.idle": "2024-04-27T11:10:59.615068Z",
          "shell.execute_reply.started": "2024-04-27T11:10:59.608612Z",
          "shell.execute_reply": "2024-04-27T11:10:59.613774Z"
        },
        "trusted": true,
        "id": "js5ed9YVbF_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "P2PJ3VLLbF_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T11:13:25.465023Z",
          "iopub.execute_input": "2024-04-27T11:13:25.465487Z",
          "iopub.status.idle": "2024-04-27T11:13:29.108383Z",
          "shell.execute_reply.started": "2024-04-27T11:13:25.465454Z",
          "shell.execute_reply": "2024-04-27T11:13:29.107234Z"
        },
        "trusted": true,
        "id": "UU7TqIcybF_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_hub_rag"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T11:13:39.588709Z",
          "iopub.execute_input": "2024-04-27T11:13:39.589767Z",
          "iopub.status.idle": "2024-04-27T11:13:39.597451Z",
          "shell.execute_reply.started": "2024-04-27T11:13:39.589687Z",
          "shell.execute_reply": "2024-04-27T11:13:39.596533Z"
        },
        "trusted": true,
        "id": "CiqZg4pMbF_E",
        "outputId": "9ab5fc6d-34c5-4b6e-b89e-460f825d4bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T11:14:25.242972Z",
          "iopub.execute_input": "2024-04-27T11:14:25.243439Z",
          "iopub.status.idle": "2024-04-27T11:14:26.938293Z",
          "shell.execute_reply.started": "2024-04-27T11:14:25.243405Z",
          "shell.execute_reply": "2024-04-27T11:14:26.937241Z"
        },
        "trusted": true,
        "id": "VxJAmjkEbF_E",
        "outputId": "b2234b8d-1f44-42f0-e889-6521e9b04161"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps, making them more manageable for an autonomous agent system. This can be achieved through prompting techniques like Chain of Thought and Tree of Thoughts, as well as task-specific instructions or human inputs.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### THE SECOND APPROACH CONSIST OF:  FROM ONE QUESTION GENERATE K QUESTION BASES ON THE FIRST ONE"
      ],
      "metadata": {
        "id": "Gn0_qSagbF_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Multi Query: Different Perspectives\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T13:29:48.641249Z",
          "iopub.execute_input": "2024-04-27T13:29:48.641834Z",
          "iopub.status.idle": "2024-04-27T13:29:48.687573Z",
          "shell.execute_reply.started": "2024-04-27T13:29:48.641756Z",
          "shell.execute_reply": "2024-04-27T13:29:48.686374Z"
        },
        "trusted": true,
        "id": "8Vu3YM30bF_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:31:48.179234Z",
          "iopub.execute_input": "2024-04-28T13:31:48.180051Z",
          "iopub.status.idle": "2024-04-28T13:31:49.744478Z",
          "shell.execute_reply.started": "2024-04-28T13:31:48.180015Z",
          "shell.execute_reply": "2024-04-28T13:31:49.743354Z"
        },
        "trusted": true,
        "id": "fQSeCrL8bF_H",
        "outputId": "767bce24-d5b9-4e66-a5f6-2391ab2f18ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n  warn_beta(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T13:34:22.645652Z",
          "iopub.execute_input": "2024-04-27T13:34:22.646094Z",
          "iopub.status.idle": "2024-04-27T13:34:26.246012Z",
          "shell.execute_reply.started": "2024-04-27T13:34:22.646063Z",
          "shell.execute_reply": "2024-04-27T13:34:26.244777Z"
        },
        "trusted": true,
        "id": "ZTspmSGPbF_I",
        "outputId": "cf4e0338-01be-4071-a424-d74c26f5fd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as Chain of Thought (CoT) and Tree of Thoughts. This allows the agent to utilize more test-time computation and enhance its performance on complex tasks by transforming big tasks into multiple manageable tasks. Task decomposition can be achieved through simple prompting, task-specific instructions, or with human inputs.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(retriever.aget_relevant_documents(\"\"))\n",
        "\n",
        "docs = retriever.invoke(\"Weng, Lilian\")\n",
        "docs[0]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T14:23:36.728217Z",
          "iopub.execute_input": "2024-04-27T14:23:36.728666Z",
          "iopub.status.idle": "2024-04-27T14:23:36.99952Z",
          "shell.execute_reply.started": "2024-04-27T14:23:36.728632Z",
          "shell.execute_reply": "2024-04-27T14:23:36.998229Z"
        },
        "trusted": true,
        "id": "I5_JNe2TbF_I",
        "outputId": "0bceb3fa-0661-4ac4-f422-e2f43786ec57"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Document(page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG-Fusion"
      ],
      "metadata": {
        "id": "DG-21DR0bF_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# RAG-Fusion: Related\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:54:03.55141Z",
          "iopub.execute_input": "2024-04-28T13:54:03.551885Z",
          "iopub.status.idle": "2024-04-28T13:54:03.558045Z",
          "shell.execute_reply.started": "2024-04-28T13:54:03.551852Z",
          "shell.execute_reply": "2024-04-28T13:54:03.556664Z"
        },
        "trusted": true,
        "id": "kPO4kCj3bF_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_rag_fusion"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:23:57.882522Z",
          "iopub.execute_input": "2024-04-28T13:23:57.882922Z",
          "iopub.status.idle": "2024-04-28T13:23:57.892323Z",
          "shell.execute_reply.started": "2024-04-28T13:23:57.882892Z",
          "shell.execute_reply": "2024-04-28T13:23:57.891121Z"
        },
        "trusted": true,
        "id": "qdie9m0LbF_J",
        "outputId": "1951b08b-abee-4aa9-c494-50cce8c0cbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='You are a helpful assistant that generates multiple search queries based on a single input query. \\n\\nGenerate multiple search queries related to: {question} \\n\\nOutput (4 queries):'))])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:54:07.957341Z",
          "iopub.execute_input": "2024-04-28T13:54:07.957753Z",
          "iopub.status.idle": "2024-04-28T13:54:07.993069Z",
          "shell.execute_reply.started": "2024-04-28T13:54:07.957723Z",
          "shell.execute_reply": "2024-04-28T13:54:07.992033Z"
        },
        "trusted": true,
        "id": "p073CewibF_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "len(docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:54:10.688155Z",
          "iopub.execute_input": "2024-04-28T13:54:10.689056Z",
          "iopub.status.idle": "2024-04-28T13:54:12.227179Z",
          "shell.execute_reply.started": "2024-04-28T13:54:10.689018Z",
          "shell.execute_reply": "2024-04-28T13:54:12.226114Z"
        },
        "trusted": true,
        "id": "b8f0myIGbF_J",
        "outputId": "47dc9387-c0a2-448e-fb89-b51b93bca513"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 53,
          "output_type": "execute_result",
          "data": {
            "text/plain": "2"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:54:17.896316Z",
          "iopub.execute_input": "2024-04-28T13:54:17.896745Z",
          "iopub.status.idle": "2024-04-28T13:54:17.929373Z",
          "shell.execute_reply.started": "2024-04-28T13:54:17.896711Z",
          "shell.execute_reply": "2024-04-28T13:54:17.928206Z"
        },
        "trusted": true,
        "id": "YKVseIcMbF_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T13:54:21.538687Z",
          "iopub.execute_input": "2024-04-28T13:54:21.539089Z",
          "iopub.status.idle": "2024-04-28T13:54:24.931869Z",
          "shell.execute_reply.started": "2024-04-28T13:54:21.539057Z",
          "shell.execute_reply": "2024-04-28T13:54:24.930764Z"
        },
        "trusted": true,
        "id": "TsfXqGpxbF_J",
        "outputId": "45109ea7-0f64-43e5-b650-ae18ee0e5de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 55,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques like Chain of Thought (CoT) and Tree of Thoughts. This allows the agent to utilize more test-time computation and enhance its performance on complex tasks by transforming big tasks into multiple manageable tasks. Task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zo9U2w7XbF_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}